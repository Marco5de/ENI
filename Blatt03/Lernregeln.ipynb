{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in die Neuroinformatik - 3. Aufgabenblatt\n",
    "## Gruppe Q: Dominik Authaler, Marco Deuscher, Carolin Schindler\n",
    "\n",
    "### Aufgabe 1: Lernregeln\n",
    "#### Teilaufgabe 1: \n",
    "Bestimmen des Gradienten der Fehlerfunktion  \n",
    "$\\nabla E(w,b) = (\\frac{\\partial E}{\\partial w},\\frac{\\partial E}{\\partial b})^T$  \n",
    "$\\frac{\\partial E}{\\partial w} = \\frac{\\partial}{\\partial w}(\\frac{1}{2}\\sum_{\\mu=1}^{M}(T_\\mu-f(wx_\\mu+b))^2)\n",
    "= -\\sum_{\\mu=1}^{M}(T_\\mu-f(wx_\\mu+b))\\cdot f'(wx_\\mu+b)\\cdot x_\\mu = \\sum_{\\mu=1}^{M}(f(wx_\\mu+b)-T_\\mu)\\cdot f'(wx_\\mu+b)\\cdot x_\\mu$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b} = \\frac{\\partial}{\\partial b}(\\frac{1}{2}\\sum_{\\mu=1}^{M}(T_\\mu-f(wx_\\mu+b))^2)\n",
    "= -\\sum_{\\mu=1}^{M}(T_\\mu-f(wx_\\mu+b))\\cdot f'(wx_\\mu+b)= \\sum_{\\mu=1}^{M}(f(wx_\\mu+b)-T_\\mu)\\cdot f'(wx_\\mu+b)\\cdot$\n",
    "\n",
    "#### Teilaufgabe 2:\n",
    "##### Inkrementelle Version\n",
    "$w(t+1) = w(t) - \\eta \\cdot(f(wx_\\mu+b)-T_\\mu)\\cdot f'(wx_\\mu+b)\\cdot x_\\mu$  \n",
    "$b(t+1) = b(t) - \\eta \\cdot(f(wx_\\mu+b)-T_\\mu)\\cdot f'(wx_\\mu+b)$\n",
    "##### Batch Version\n",
    "$w(t+1) = w(t) - \\eta\\cdot\\;(1\\quad 0)\\;\\nabla E(w,b) = w(t) - \\eta \\cdot \\sum_{\\mu=1}^{M}(f(wx_\\mu+b)-T_\\mu)\\cdot f'(wx_\\mu+b)\\cdot x_\\mu$  \n",
    "$b(t+1) = b(t) - \\eta\\cdot\\;(0\\quad 1)\\;\\nabla E(w,b) = b(t) - \\eta \\cdot \\sum_{\\mu=1}^{M}(f(wx_\\mu+b)-T_\\mu)\\cdot f'(wx_\\mu+b)$\n",
    "\n",
    "#### Teilaufgabe 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad_w0: 0.36260270307341996\n",
      "Grad_b0: 0.25141536222290073\n",
      "New weight: -1.290082162458736\n",
      "New bias: 2.7988677102216792\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return math.exp(-x)/(1+math.exp(-x))**2\n",
    "\n",
    "def grad_E(x,T,w0,b0):\n",
    "    grad_w = 0\n",
    "    grad_b = 0\n",
    "    for i in range(len(x)):\n",
    "        grad_w += (sigmoid(x[i]*w0+b0)-T[i]) * d_sigmoid(w0*x[i]+b0)*x[i]\n",
    "        grad_b += (sigmoid(x[i]*w0+b0)-T[i]) * d_sigmoid(w0*x[i]+b0)\n",
    "    return grad_w,grad_b\n",
    "\n",
    "def batch_version(w0,b0,grad_w,grad_b,eta):\n",
    "    return w0-eta*(grad_w), b0-eta*(grad_b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w0 = -1\n",
    "b0 = 3\n",
    "eta = 0.8\n",
    "x = [-1, 0, 1, 2]\n",
    "T = [0, 1, 0, 0]\n",
    "\n",
    "grad_w0 = 0 #grad start pos\n",
    "grad_b0 = 0 #grad start pos\n",
    "w1 = 0 #weight nach dem ersten update\n",
    "b1 = 0 #bias nach dem ersten update\n",
    "\n",
    "grad_w0,grad_b0 = grad_E(x,T,w0,b0)\n",
    "print(\"Grad_w0: \" + str(grad_w0))\n",
    "print(\"Grad_b0: \" + str(grad_b0))\n",
    "w1,b1 = batch_version(w0,b0,grad_w0,grad_b0,eta)\n",
    "print(\"New weight: \" + str(w1))\n",
    "print(\"New bias: \" + str(b1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) ![title](b03.png)\n",
    "(c) siehe cell above  \n",
    "(d) Eigenschaften des Gradienten sind identisch für lokale und globale Minima. Daher kann man mit dem Gradientenabstiegsverfahren i.A. nur lokale Minima finden.  \n",
    "\n",
    "#### Teilaufgabe 4:\n",
    "Bei der Batch Variante wird immer ein vollständiges Dataset verwendet um den Gradienten und dann daraus die neuen Gewichte und Bias zu berechenen. Bei der inkrementellen Version geschieht dieser Prozess nach jedem der Datenpunkte.  \n",
    "Pfad 1 stammt von der Batch Version und Pfad 2 von der inkrementellen Version. Dies lässt gut an den in Pfad 2 enthaltenen Zacken erkennen.  \n",
    "\n",
    " \n",
    "Batches sind effizienter in der Berechnung, da nicht nach jedem Punkt die Gewichte und der Bias angepasst werden muss. Außerdem wird durch die Summierung über merhere Datenpunkte ein Mittelwert gebildet und man erhält damit einen zuverlässigen Gradienten.\n",
    "Ein Vorteil der inkrementellen Methode ist, dass nicht immer das gesamte Dataset zum trainieren verwendet werden muss. Außerdem findet das inkrementelle Verfahren schneller ein Minimum und mit höherer Wahrscheinlichkeit ein globales. Da beim der inkrementellen Methode immer nur ein einziger Datenpunkt verwndet wird, kann es vorkommen, dass ein lokales Minimum übersprugnen wird. Dies kann unter anderem auch dafür verwender werden, um nicht gegen einen Sattelpunkt zu konvergieren.\n",
    "\n",
    "#### Teilaufgabe 5:\n",
    "Wenn die Lernrate zu groß wird, kann es vorkommen, dass ein lokales Minimum übersprungen wird und man nicht gegen dieses konvergiert. \n",
    "$w(t+1) = w(t) - \\eta\\;(1\\quad 0)\\;\\nabla E(w,b)$  \n",
    "Ist die Lernrate zu groß, hat auch ein kleiner Gradient noch eine merkliche Auswirkung auf die Gewichte.  \n",
    "In der Animation kann man gut sehen, dass wenn die Lernrate zu groß wird, die Sprünge zwischen den einzelnen Punkten zu groß werden.\n",
    "\n",
    "#### Teilaufgabe 6:\n",
    "Man muss beachten, dass das von uns beschriebene Netz nur aus einem einzelnen Neuron besteht. Als Transferfunktion wurde die Sigmoid-Funktion gewählt. Folglich kann der Output des Netzes auch immer nur die Form einer solchen Sigmoid-Funktion haben. Mit den gewählten Parametern kann diese etwas gestaucht,gestreckt oder verschoben werden aber nicht grundsätzliche verändert werden.  \n",
    "Die Approximation ist im Minimum so gut wie es die Umgebung des Anfangspunktes zugelassen hat, d.h. aber nicht, dass das erhaltene Ergebnis auch sinnvoll ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
