{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einf√ºhrung in die Neuroinformatik - 10. Aufgabenblatt\n",
    "## Gruppe Q: Dominik Authaler, Marco Deuscher, Carolin Schindler\n",
    "\n",
    "### Aufgabe 1: Transferfunktionen\n",
    "\n",
    "#### 1. Sigmoidfunktion und Tangens hyperbolicus\n",
    "##### a) Zeige, dass $sig(x) = \\frac{1+\\tanh(\\frac{x}{2})}{2}$.\n",
    "$sig(x) = \\frac{1}{1+e^{-x}} = \\frac{2\\cdot e^{\\frac{x}{2}}}{2\\cdot e^{\\frac{x}{2}} + 2\\cdot e^{-\\frac{x}{2}}} = \\frac{e^{\\frac{x}{2}} + e^{-\\frac{x}{2}} + e^{\\frac{x}{2}} - e^{-\\frac{x}{2}}}{2\\cdot (e^{\\frac{x}{2}} + e^{-\\frac{x}{2}})} = \\frac{1+\\frac{e^{\\frac{x}{2}} - e^{-\\frac{x}{2}}}{e^{\\frac{x}{2}} + e^{-\\frac{x}{2}}}}{2} = \\frac{1+\\tanh(\\frac{x}{2})}{2}$\n",
    "##### b) Unterschiede beim Lernen\n",
    "###### i) Markiere f√ºr jede Verbindung in Abbildung 4(a) f√ºr $f(x)=sig(x)$, ob der Gradient des zugeh√∂rigen Gewichts positiv oder negativ ist.\n",
    "gr√ºn: Gradient positiv, rot: Gradient negativ  \n",
    "<img src=\"Blatt10_Bild1bi.png\">\n",
    "###### ii) Wiederhole das Verfahren f√ºr das Netzwerk in Abblidung 4(b) mit $f(x) = \\tanh(x)$ als Transferfunktion.\n",
    "gr√ºn: Gradient positiv, rot: Gradient negativ  \n",
    "<img src=\"Blatt10_Bild1bii.png\">\n",
    "###### iii) Erkl√§re, wie sich die beiden Netzwerke beim Lernen unterscheiden und welchen Nachteil die Sigmoidfunktion im Vergleich zum Tangens hyperbolicus haben k√∂nnte.\n",
    "Der Tangens hyperbolicus ist punktsymmetrisch um den Ursprung, die Sigmoidfunktion besitzt keine Symmetrie bez√ºglich des Ursprungs und ist f√ºr alle Werte gr√∂√üer Null. Die Sigmoidfunktion k√∂nnte das Problem von vanishing Gradients haben, da sie f√ºr Wert $<-4$ nahezu Null ist und somit die Gradienten sehr klein werden. Zudem beeinflusst die Sigmoidfunktion nicht die Richtung des Gradienten, sondern nur dessen Betrag.\n",
    "\n",
    "#### 2. Rectified Linear Units (ReLUs)\n",
    "##### a) Welche Konsequenzen hat die Funktion auf das Lernen im Netzwerk?\n",
    "$\\forall x<0: \\text{ReLU}(x) = 0 \\land \\text{ReLU}'(x) = 0$  \n",
    "$\\forall x>0: \\text{ReLU}(x) = x \\land \\text{ReLU}'(x) = 1$  \n",
    "D.h. f√ºr negative x ist der Gradient Null und somit bleiben die Gewichte beim Lernen unver√§ndert, obwohl diese nicht korrekt sind. Dadurch wird das Lernen verlangsamt, da nur f√ºr positive x gelernt werden kann.\n",
    "##### b) Vergleiche die Performance und die Geschwindigkeit des Lernvorgangs zwischen ReLU(x) und tanh(x).\n",
    "ReLU besitzt einen geringeren Rechenaufwand als tanh, da f√ºr tanh zwei e-Funktionen berechnet, addiert bzw. subtrahiert und geteil werden m√ºssen w√§hrend bei ReLU nur x positiv oder negativ unterschieden wird und je nachdem die Ausgabe 0 oder x ist.  \n",
    "Der Lernvorgang ist bei tanh(x) schneller, als bei ReLU(x), da bei tanh f√ºr positive und negative x gelernt wird und bei ReLU nur f√ºr positive x gelernt wird (siehe a).\n",
    "##### c) Erkl√§re, was das Problem bei dying ReLUs ist und warum eine Leaky ReLU m√∂gliche Abhilfe darstellt.\n",
    "Das Problem ist hier, dass ReLU(x) = 0 und somit wie in Teilaufgabe a) beschrieben nicht gelernt wird. Eine Leaky ReLU stellt eine m√∂gliche Abhilfe dar, da $\\forall x\\neq 0: \\text{LeakyReLU}_a(x) \\neq 0$ und somit nur nicht gelernt wird, wenn $x=0$.\n",
    "\n",
    "#### 3. Exponential Linear Unit (ELU)\n",
    "##### a) Welchen Vorteil bringt die Funktion f√ºr das Lernen im Netzwerk?\n",
    "Bei der ReLU springt die Ableitung an der Stelle Null von 0 auf 1 (f√ºr $\\epsilon > 0$ gitl ReLU($\\epsilon \\rightarrow 0$) = 1 und f√ºr $\\epsilon < 0$ gilt ReLU($\\epsilon \\rightarrow 0$) = 0) und somit springt der Gradient der Fehlerfunktion an dieser Stelle auch von 0 auf einen Wert ungleich 0. F√ºr den Lernvorgang bedeutet dies einen Sprung von nichts Lernen auf Lernen. Bei der ELU gibt es keine Spr√ºnge in der Ableitung, d.h. das Lernen steigt langsam an und findet nicht aufgrund infinitesimaler Unterschiede garnicht statt.\n",
    "##### b) Scaled Exponential Linear Unit (SELU)\n",
    "###### i) Warum k√∂nnte es ung√ºnstig sein, dass die Verteilungsparameter √ºber die Lernepochen hinweg so unterschiedliche sind? Welche Nachteile ergeben sich hierbei insbesondere f√ºr die $l+1$ Schicht?\n",
    "Beim Lernen sollen die Gewichte gegen einen Wert konvergieren. Variieren nun die Verteilungsparameter der Aktivit√§ten der l-ten Schicht von Epoche zu Epoche, so variiert aufgrund der Backpropagation auch die Gewichtsanpassung der Gewichte der l+1 Schicht von Epoche zu Epoche und kann nicht konvergieren, wodurch das \"Lernziel\" nicht erreicht wird.\n",
    "###### ii) Wie transformiert SELU(x) diese Werte, sodass dadurch die neue Verteilung in Abbildung 5(b) entsteht?\n",
    "F√ºr $u \\geq 0$ ist $\\text{SELU}(u) \\approx u$. Daher ist die Verteilung von u und SELU(u) f√ºr $u \\geq 0$ in etwa gleich.  \n",
    "F√ºr $u<0$ ist $\\text{SELU}(u) \\approx 1.67\\cdot(e^u-1)$ und somit ist $\\lim_{u\\rightarrow -\\infty} \\text{SELU}(u) \\approx -1.67$. Diese Konvergenz tritt relativ schnell ein: SELU(-1) $\\approx$ -1.056; SELU(-2) $\\approx$ -1.44; SELU(-3) $\\approx$ -1.59; SELU(-4) $\\approx$ -1.63  \n",
    "Deshalb kommt SELU(u) kleiner etwa -1.67 nie vor und Wahrscheinlichkeit, dass $SELU(u) \\approx -1.67$ ist aufgrund der Verteilung von $u$ sehr gro√ü.\n",
    "###### iii) Erkl√§re auch hier, wie es zu der neuen Verteilung in Abbildung 6(b) kommt.\n",
    "F√ºr $u \\geq 0$ ist $\\text{SELU}(u) \\approx u$. Daher ist die Verteilung von u und SELU(u) f√ºr $u \\geq 0$ in etwa gleich.  \n",
    "F√ºr $u<0$ ist $\\text{SELU}(u) \\approx 1.67\\cdot(e^u-1)$ und somit ist $\\lim_{u\\rightarrow -\\infty} \\text{SELU}(u) \\approx -1.67$. Da u fast nie kleiner als -1 ist, kommt SELU(u) kleiner $\\text{SELU}(-1)\\approx -1.056$ fast nie vor. Im Bereich $-1<u<0$ konvergiert SELU(u) langsam gegen ihren Endwert, weshalb die Verteilung nicht wie oben einen Peak am Endwert aufweist, sondern aufgrund der Verteilung von $u$ die Wahrscheinlichkeit Richtung Endwert absinkt.\n",
    "#### 4. spielerisches Testen im TensorFlow-Playground\n",
    "##### a) Wie viele Epochen sind jeweils notwendig, um den Testfehler auf ca. 0.007 zu dr√ºcken?\n",
    "$sig(x)$: 1286 Epochen  \n",
    "<img src=\"Blatt10_Bild4a_sig.png\">  \n",
    "$\\tanh(x)$: 207 Epochen  \n",
    "<img src=\"Blatt10_Bild4a_tanh.png\">  \n",
    "$\\text{ReLU}(x)$: 109 Epochen  \n",
    "<img src=\"Blatt10_Bild4a_relu.png\">  \n",
    "##### b) Kannst du den Effekt der dying ReLUs provozieren?\n",
    "<img src=\"Blatt10_Bild4b.png\"> \n",
    "\n",
    "#### 5. Simulationsergebnisse untersuchen\n",
    "##### a) Verteilungsparamteter f√ºr die Aktivit√§ten und die Gradienten\n",
    "###### i) Was f√§llt bez√ºglich des Bereichs auf?\n",
    "Die Varianz der Aktivit√§ten und Gradienten ist mit der SELU Funktion deutlich gr√∂√üer als mit den anderen drei Transferfunktionen. Zudem f√§llt bei den Aktivi√§ten auf, dass die Varianz in h√∂heren Layern f√ºr SELU gegen 1 w√§chst, w√§hrend f√ºr die anderen drei Funktionen die Varianz gegen 0 f√§llt. Bei den Gradienten ist auff√§llig, dass sich mit zunehmdem Layer der Mittelwert bei der SELU Funktion (im Gegensatz zu den anderen drei Funktionen) von 0 unterscheidet\n",
    "###### ii) Wie macht sich die Asymmetrie der ReLU- und ELU-Funktionen bemerkbar?\n",
    "Bei der Aktivierung ist bei ReLU und ELU f√ºr niedrige Layer der Mittelwert positiv (insbesondere bei ReLU) bevor dieser f√ºr weitere Schichten gegen 0 geht. Dadurch ist die Verteilung der Aktivierung f√ºr niedrige Layer nicht wie bei tanh oder SELU symmetrisch um Null.\n",
    "###### iii) Warum ist der learning slowdown bei den Gradienten f√ºr tanh(ùë•) nicht so stark ausgepr√§gt?\n",
    "Dies liegt daran, dass die Varianz der Gradienten f√ºr $\\tanh x$ gr√∂√üer ist als ReLU und SELU und somit betragsm√§√üig gr√∂√üere Gradienten enstehen k√∂nnen.\n",
    "###### iv) Mit welcher Transferfunktion wird in diesem Beispiel am meisten gelernt?\n",
    "Mit SELU als Transferfunktion, da dort der Gradient aufgrund der Varianz betragsm√§√ügig am gr√∂√üten ist und je gr√∂√üer der Gradient betragsm√§√üig, desto st√§rker werden die Gewichte angepasst und desto mehr wird gelernt.\n",
    "##### b) Beschreibe Breite und Form der Verteilungen von der ersten bis zur letzten Schicht und erkl√§re, wie diese durch die jeweilige Transferunktion entsteht.\n",
    "Die Aktivit√§t von $\\tanh x$ ist symmetrisch um die Null und geht f√ºr h√∂here Schichten gegen Null. Die Symmetrie ergibt sich aus der Symmetrie der $\\tanh$-Funktion.  \n",
    "Die Aktivit√§t von ReLU(x) geht f√ºr h√∂here Schichten gegen Null. Die Verteilung ist schm√§ler als bei $\\tanh x$, was daran liegt, dass ReLU(x) f√ºr negative x (die H√§lfte der x in der Standardverteilung) Null ist.  \n",
    "Die Aktivit√§t von ELU(x) geht f√ºr h√∂here Schichten gegen Null und das st√§rker als $\\tanh x$, da ELU(x) f√ºr positive x st√§rker ansteigt als $\\tanh x$.  \n",
    "Die Aktivit√§t von SELU(x) ist deutlich breiter Verteilt als die der anderen drei Funktionen. Zudem ist die Verteilung √ºber alle Layer nahezu konstant. Dies liegt daran, dass die SELU-Funktion f√ºr $|x|<1$ den gr√∂√üten Bildbereich von allen Funktionen hat und wie oben untersucht normalisierend wirkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
