{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Einführung in die Neuroinformatik - Übungsblatt 5</center></h1>\n",
    "<h2><center>Dominik Authaler, Marco Deuscher und Carolin Schindler</center></h2>\n",
    "<h2><center>Gruppe Q</center></h2>\n",
    "<h2><center>Juni 2019</center></h2>\n",
    "\n",
    "## Aufgabe 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\hline\n",
    "\\text{Schritt} & \\text{Berechnung} & \\text{Benötigte Variablen} & \\text{Phase} \\\\\n",
    "\\hline\n",
    "1 & u_1^{(1)} & x_1, w_{11}^{(1)}, x_2, w_{21}^{(1)}& \\text{I}\\\\\n",
    "2 & y_1^{(1)}& u_1^{(1)}, b_1^{(1)}& \\text{I}\\\\\n",
    "3 & u_1^{(2)}& y_1^{(1)}, w_{11}^{(2)}, y_2^{(1)}, w_{21}^{(2)}& \\text{I}\\\\\n",
    "4 & y_1^{(2)}& u_1^{(2)}, b_1^{(2)}& \\text{I}\\\\\n",
    "5 & \\delta_1^{(2)}& y_1^{(2)}, T, u_1^{(2)}& \\text{II}\\\\\n",
    "6 & \\delta_1^{(1)}& w_{11}^{(2)}, \\delta_1^{(2)}, u_1^{(1)}& \\text{III}\\\\\n",
    "7 & w_{11}^{(1)}& x_1, x_2, \\eta, \\delta_1^{(1)} & \\text{IV}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Backpropagation selbst implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_deriv(inp):\n",
    "    return 1 / (np.cosh(inp)**2)\n",
    "\n",
    "def forward(inpX, w1, w2, b1, b2):\n",
    "    '''\n",
    "    Perform a forward step of the network. For the transfer function in the hidden layer, use tanh.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inpX : data matrix\n",
    "        input matrix, shaped as: samples x dimensions\n",
    "    w1 : matrix\n",
    "        weight matrix between input and hidden neurons\n",
    "    w2 : matrix\n",
    "        weight matrix between hidden and output neurons\n",
    "    b1 : vector\n",
    "        bias vector for the hidden neurons\n",
    "    b2 : vector\n",
    "        bias vector for the output neurons\n",
    "    '''\n",
    "    \n",
    "    u1 = np.add(np.dot(inpX, w1), b1)\n",
    "    y1 = np.tanh(u1)\n",
    "    y2 = np.asscalar(np.add(np.dot(y1, w2), b2))\n",
    "    \n",
    "    #print(inpX.shape)\n",
    "    #print(w1.shape)\n",
    "    #print(w2.shape)\n",
    "    #print(b1.shape)\n",
    "    #print(b2.shape)\n",
    "    \n",
    "    #print(\"Res: \")\n",
    "    #print(y1.shape)\n",
    "    \n",
    "    return u1, y1, y2\n",
    "\n",
    "def initialize_weights(inpDim, hiddenNeurons, outDim):\n",
    "    '''\n",
    "    Initialize the weight matrix based on input Dimension, amount of hidden neurons and output dimension.\n",
    "    The range for the initial weights is given by [-.5; .5].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inpDim : int\n",
    "        Number of input neurons\n",
    "    hiddenNeurons : int\n",
    "        Number of hidden neurons\n",
    "    outDim : int\n",
    "        Number of output neurons\n",
    "    weights : list\n",
    "        List containing the weights and biases in the following order: [w1, w2, b1, b2]\n",
    "    '''\n",
    "    \n",
    "    w1 = np.subtract(np.random.rand(inpDim, hiddenNeurons), 0.5)\n",
    "    \n",
    "    w2 = np.subtract(np.random.rand(hiddenNeurons, outDim), 0.5)\n",
    "    \n",
    "    b1 = np.subtract(np.random.rand(1, hiddenNeurons), 0.5)\n",
    "    \n",
    "    b2 = np.subtract(np.random.rand(1, outDim), 0.5)\n",
    "    \n",
    "    #print(w1.shape)\n",
    "    #print(w2.shape)\n",
    "    #print(b1.shape)\n",
    "    #print(b2.shape)\n",
    "    \n",
    "    weights = [w1, w2, b1, b2];\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def prop_error(T, y2, w2, transDiff_u1):\n",
    "    '''\n",
    "    Calculation of the error of the network\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : float\n",
    "        teaching signal of the current sample\n",
    "    y2 : float\n",
    "        output of the last neuron\n",
    "    w2 : data matrix\n",
    "        weight matrix between hidden and output layer\n",
    "    transDiff_u1 : vector\n",
    "        differential of the transfer function used on u1\n",
    "    '''\n",
    "    \n",
    "    # Ableitung von Transferfunktion f(x) = x ist 1\n",
    "    delta2 = (T-y2);\n",
    "    \n",
    "    delta1 = w2.transpose()*delta2*transDiff_u1\n",
    "    \n",
    "    return delta1, delta2\n",
    "\n",
    "def training(hiddenNeurons, lernRate, inpX_T, inpDim, epoch):\n",
    "    '''\n",
    "    Train the neural network. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hiddenNeurons : int\n",
    "        Number of hidden Neurons\n",
    "    lernRate : float\n",
    "        Lernrate \\eta\n",
    "    inpX_T : combined matrix of the inputData and the teaching signal\n",
    "        input data and shaped as: samples x dimensions \n",
    "    inpDim : dimension of the inputData\n",
    "    epoch : int\n",
    "        number of training epochs\n",
    "    '''\n",
    "    \n",
    "    #print(inpX_T.shape)\n",
    "    [length, dim] = inpX_T.shape\n",
    "    \n",
    "    print('Training des Netzes mit ' + str(length) + \" Lehrersignalen über \" + str(epoch) + \" Epochen\")\n",
    "    \n",
    "    [w1, w2, b1, b2] = initialize_weights(inpDim, hiddenNeurons, dim - inpDim)\n",
    "    errorList = []\n",
    "    \n",
    "    for e in range (epoch):\n",
    "    #for e in range(1,2):\n",
    "        inpPerm = np.random.permutation(inpX_T)\n",
    "        for i in range (0, length):\n",
    "        #for i in range (0, 1):\n",
    "            samplePoint = inpPerm[[i],:]\n",
    "            #print(samplePoint)\n",
    "            inpX = samplePoint[:, (0,1)]\n",
    "            T = np.asscalar(samplePoint[:, 2])\n",
    "\n",
    "            [u1, y1, y2] = forward(inpX, w1, w2, b1, b2)\n",
    "            [d1, d2] = prop_error(T, y2, w2, tanh_deriv(u1))\n",
    "\n",
    "            #print(\"y1: \" + str(y1.shape))\n",
    "            #print(\"w1: \" + str(w1.shape))\n",
    "            #print(\"d1: \" + str(d1.shape))\n",
    "            #print(\"d2: \" + str(d2.shape))\n",
    "            #print(\"x: \" + str(samplePoint.shape))\n",
    "            #print(\"b1: \" + str(b1.shape))\n",
    "            #print(\"b2: \" + str(b2.shape))\n",
    "\n",
    "            #Lernschritt\n",
    "            w2 = np.add(w2, lernRate*y1.transpose()*d2)\n",
    "            w1 = np.add(w1, lernRate*inpX.transpose()*d1)\n",
    "\n",
    "            b2 = b2 + lernRate*d2\n",
    "            b1 = b1 + lernRate*d1\n",
    "\n",
    "\n",
    "        ## Netzwerkfehler berechnen\n",
    "        error = 0\n",
    "        for i in range (0, length):\n",
    "            samplePoint = inpPerm[[i],:]\n",
    "            inpX = samplePoint[:, (0,1)]\n",
    "            T = np.asscalar(samplePoint[:, 2])\n",
    "\n",
    "            [u1, y1, y2] = forward(inpX, w1, w2, b1, b2)\n",
    "\n",
    "            #print(\"T: \" + str(T))\n",
    "            #print(\"Y2: \" + str(y2))\n",
    "\n",
    "            error += (T-y2)**2\n",
    "\n",
    "        #print(\"ERROR: \" + str(error))\n",
    "        errorList.append(error)\n",
    "    return [errorList, w1, w2, b1, b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialiserung der Parameter und Training:\n",
    "- X und Y entsprechen dem Datensatz\n",
    "- Z ist das Lehrersignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training des Netzes mit 900 Lehrersignalen über 1000 Epochen\n",
      "Anfangsfehler: 597.0223765057898\n",
      "Endfehler:     3.7847749993400206\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## Generate some sample data\n",
    "def f(x,y):\n",
    "    return np.sin(np.sqrt(x**2 + y**2)) + np.cos(.9*(x-y))\n",
    "\n",
    "X = np.linspace(-6, 6, 30)\n",
    "Y = np.linspace(-6, 6, 30)\n",
    "x, y = np.meshgrid(X, Y)\n",
    "z = f(x, y)\n",
    "\n",
    "# Change the shape of the data point\n",
    "inpX_T = np.reshape(np.dstack([x,y,z]), (900, 3))\n",
    "#print(outT.shape)\n",
    "#print(inpX.shape)\n",
    "\n",
    "############\n",
    "## Initialize network parameter\n",
    "inputNeuronen = 2\n",
    "hiddenNeuronen = 100\n",
    "lernRate       = 0.01\n",
    "epochen        = 1000\n",
    "outputNeuronen = 1;\n",
    "\n",
    "#############\n",
    "## Start training of the neural network\n",
    "[errors, w1, w2, b1, b2] = training(hiddenNeuronen, lernRate, inpX_T, inputNeuronen, epochen)\n",
    "#print(len(errors))\n",
    "print(\"Anfangsfehler: \" + str(errors[0]))\n",
    "print(\"Endfehler:     \" + str(errors[len(errors) - 1]))\n",
    "#print(\"Fehlerentwicklung: \" + str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f16504cfca44fb9a1e59a35f149cdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anzuzeigende Daten erzeugen\n",
    "approx = np.zeros(x.shape)\n",
    "diff = np.zeros(x.shape)\n",
    "\n",
    "[len1, len2] = x.shape\n",
    "for a in range(0, len1):\n",
    "    for b in range(0, len2):\n",
    "        inpX = [x[a][b], y[a][b]];\n",
    "        #print(inpX)\n",
    "        [_, _, y2] = forward(inpX, w1, w2, b1, b2)\n",
    "        approx[a][b] = y2\n",
    "        diff[a][b] = f(x[a][b], y[a][b]) - y2\n",
    "\n",
    "\n",
    "# Daten visualisieren\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2,2,1, projection='3d')\n",
    "surf1 = ax1.plot_surface(x,y,z, cmap = 'plasma',\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax1.set_title('Original')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('Original');\n",
    "\n",
    "ax2 = fig.add_subplot(2,2,2, projection='3d')\n",
    "surf2 = ax2.plot_surface(x,y,approx, cmap = 'plasma',\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax2.set_title('Approximation')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('Approximation');\n",
    "\n",
    "ax3 = fig.add_subplot(2,2,3, projection='3d')\n",
    "surf3 = ax3.plot_surface(x,y,diff, cmap = 'plasma',\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax3.set_title('Differenz')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_zlabel('Differenz');\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(\"Quadratischer Fehler über die Epochen\")\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Fehler')\n",
    "plt.plot(np.arange(0, len(errors), 1), errors)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
