{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einf√ºhrung in die Neuroinformatik - 10. Aufgabenblatt\n",
    "## Gruppe Q: Dominik Authaler, Marco Deuscher, Carolin Schindler\n",
    "\n",
    "### Aufgabe 1: Transferfunktionen\n",
    "\n",
    "#### 1. Sigmoidfunktion und Tangens hyperbolicus\n",
    "##### a) Zeige, dass $sig(x) = \\frac{1+\\tanh(\\frac{x}{2})}{2}$.\n",
    "$sig(x) = \\frac{1}{1+e^{-x}} = \\frac{2\\cdot e^{\\frac{x}{2}}}{2\\cdot e^{\\frac{x}{2}} + 2\\cdot e^{-\\frac{x}{2}}} = \\frac{e^{\\frac{x}{2}} + e^{-\\frac{x}{2}} + e^{\\frac{x}{2}} - e^{-\\frac{x}{2}}}{2\\cdot (e^{\\frac{x}{2}} + e^{-\\frac{x}{2}})} = \\frac{1+\\frac{e^{\\frac{x}{2}} - e^{-\\frac{x}{2}}}{e^{\\frac{x}{2}} + e^{-\\frac{x}{2}}}}{2} = \\frac{1+\\tanh(\\frac{x}{2})}{2}$\n",
    "##### b) Unterschiede beim Lernen\n",
    "###### i) Markiere f√ºr jede Verbindung in Abbildung 4(a) f√ºr $f(x)=sig(x)$, ob der Gradient des zugeh√∂rigen Gewichts positiv oder negativ ist.\n",
    "gr√ºn: Gradient positiv, rot: Gradient negativ  \n",
    "<img src=\"Blatt10_Bild1bi.png\">\n",
    "###### ii) Wiederhole das Verfahren f√ºr das Netzwerk in Abblidung 4(b) mit $f(x) = \\tanh(x)$ als Transferfunktion.\n",
    "gr√ºn: Gradient positiv, rot: Gradient negativ  \n",
    "<img src=\"Blatt10_Bild1bii.png\">\n",
    "###### iii) Erkl√§re, wie sich die beiden Netzwerke beim Lernen unterscheiden und welchen Nachteil die Sigmoidfunktion im Vergleich zum Tangens hyperbolicus haben k√∂nnte.\n",
    "Der Tangens hyperbolicus ist punktsymmetrisch um den Ursprung, die Sigmoidfunktion besitzt keine Symmetrie bez√ºglich des Ursprungs und ist f√ºr alle Werte gr√∂√üer Null. Die Sigmoidfunktion k√∂nnte das Problem von vanishing Gradients haben, da sie f√ºr Wert $<-4$ nahezu Null ist und somit die Gradienten sehr klein werden. Zudem beeinflusst die Sigmoidfunktion nicht die Richtung des Gradienten, sondern nur dessen Betrag.\n",
    "\n",
    "#### 2. Rectified Linear Units (ReLUs)\n",
    "##### a) Welche Konsequenzen hat die Funktion auf das Lernen im Netzwerk?\n",
    "$\\forall x<0: \\text{ReLU}(x) = 0 \\land \\text{ReLU}'(x) = 0$  \n",
    "$\\forall x>0: \\text{ReLU}(x) = x \\land \\text{ReLU}'(x) = 1$  \n",
    "D.h. f√ºr negative x ist der Gradient Null und somit bleiben die Gewichte beim Lernen unver√§ndert, obwohl diese nicht korrekt sind. Dadurch wird das Lernen verlangsamt, da nur f√ºr positive x gelernt werden kann.\n",
    "##### b) Vergleiche die Performance und die Geschwindigkeit des Lernvorgangs zwischen ReLU(x) und tanh(x).\n",
    "ReLU besitzt einen geringeren Rechenaufwand als tanh, da f√ºr tanh zwei e-Funktionen berechnet, addiert bzw. subtrahiert und geteil werden m√ºssen w√§hrend bei ReLU nur x positiv oder negativ unterschieden wird und je nachdem die Ausgabe 0 oder x ist.  \n",
    "Der Lernvorgang ist bei tanh(x) schneller, als bei ReLU(x), da bei tanh f√ºr positive und negative x gelernt wird und bei ReLU nur f√ºr positive x gelernt wird (siehe a).\n",
    "##### c) Erkl√§re, was das Problem bei dying ReLUs ist und warum eine Leaky ReLU m√∂gliche Abhilfe darstellt.\n",
    "Das Problem ist hier, dass ReLU(x) = 0 und somit wie in Teilaufgabe a) beschrieben nicht gelernt wird. Eine Leaky ReLU stellt eine m√∂gliche Abhilfe dar, da $\\forall x\\neq 0: \\text{LeakyReLU}_a(x) \\neq 0$ und somit nur nicht gelernt wird, wenn $x=0$.\n",
    "\n",
    "#### 3. Exponential Linear Unit (ELU)\n",
    "##### a) Welchen Vorteil bringt die Funktion f√ºr das Lernen im Netzwerk?\n",
    "Bei der ReLU springt die Ableitung an der Stelle Null von 0 auf 1 (f√ºr $\\epsilon > 0$ gilt ReLU($\\epsilon \\rightarrow 0$) = 1 und f√ºr $\\epsilon < 0$ gilt ReLU($\\epsilon \\rightarrow 0$) = 0) und somit springt der Gradient der Fehlerfunktion an dieser Stelle auch von 0 auf einen Wert ungleich 0. F√ºr den Lernvorgang bedeutet dies einen Sprung von nichts Lernen auf Lernen. Bei ELU gibt es keine Spr√ºnge in der Ableitung, d.h. das Lernen steigt langsam an und findet nicht aufgrund infinitesimaler Unterschiede garnicht statt.\n",
    "##### b) Scaled Exponential Linear Unit (SELU)\n",
    "###### i) Warum k√∂nnte es ung√ºnstig sein, dass die Verteilungsparameter √ºber die Lernepochen hinweg so unterschiedliche sind? Welche Nachteile ergeben sich hierbei insbesondere f√ºr die $l+1$ Schicht?\n",
    "XXX\n",
    "###### ii) Wie transformiert SELU(x) diese Werte, sodass dadurch die neue Verteilung in Abbildung 5(b) entsteht?\n",
    "XXX\n",
    "###### iii) Erkl√§re auch hier, wie es zu der neuen Verteilung in Abbildung 6(b) kommt.\n",
    "XXX\n",
    "\n",
    "#### 4. spielerisches Testen im TensorFlow-Playground\n",
    "##### a) Wie viele Epochen sind jeweils notwendig, um den Testfehler auf ca. 0.007 zu dr√ºcken?\n",
    "$sig(x)$: 1286 Epochen  \n",
    "<img src=\"Blatt10_Bild4a_sig.png\">  \n",
    "$\\tanh(x)$: 207 Epochen  \n",
    "<img src=\"Blatt10_Bild4a_tanh.png\">  \n",
    "$\\text{ReLU}(x)$: 109 Epochen  \n",
    "<img src=\"Blatt10_Bild4a_relu.png\">  \n",
    "##### b) Kannst du den Effekt der dying ReLUs provozieren?\n",
    "<img src=\"Blatt10_Bild4b.png\"> \n",
    "\n",
    "#### 5. Simulationsergebnisse untersuchen\n",
    "##### a) Verteilungsparamteter f√ºr die Aktivit√§ten und die Gradienten\n",
    "###### i) Was f√§llt bez√ºglich des Bereichs auf?\n",
    "W√§hrend die SELU-Transferfunktion relativ konstant f√ºr alle Schichten den Mittelwert bei $\\mu = 0$ und die Standardabweichung bei $\\sigma = 1$ h√§lt, ist bei den anderen Transferfunktionen lediglich der Mittelwert konstant. Die Standardabweichung nimmt dagegen mit jeder Schicht weiter ab.  \n",
    "Bei den Gradienten bleiben die Kurven von tanh, ReLU und ELU sehr nah um die x-Achse verteilt, w√§hrend lediglich die Kurve von SELU sowohl vom Mittelwert her abdriftet, als auch eine gr√∂√üere Standardabweichung aufweist.  \n",
    "###### ii) Wie macht sich die Asymmetrie der ReLU- und ELU-Funktionen bemerkbar?\n",
    "Die Asymmetrie der ReLU und ELU Funktionen macht sich auf zweierlei Weisen bemerkbar. Zum Einen ist ihr Mittelwert etwas in den positiven Bereich verschoben, und zum Anderen ist der Bereich ihrer Standardabweichung asymetrisch. Dieser Bereich ist nach oben (in die positive Richtung) deutlich kleiner ausgepr√§gt, als nach unten. An der ReLU-Kurve zeigt sich dieser Effekt am st√§ksten, bei der Kurve von ELU ist er etwas schw√§cher ausgepr√§gt.  \n",
    "###### iii) Warum ist der learning slowdown bei den Gradienten f√ºr tanh(ùë•) nicht so stark ausgepr√§gt?\n",
    "XXX\n",
    "###### iv) Mit welcher Transferfunktion wird in diesem Beispiel am meisten gelernt?\n",
    "Der gr√∂√üte Lerneffekt wird in diesem Beispiel mit der SELU-Funktion erzielt, da bei den anderen Transferfunktionen (wie unter i) schon beschrieben wurde) die Gradienten sehr klein sind. Entsprechend f√§llt hier auch die √Ñnderung der Gewichte deutlich kleiner aus als bei SELU, entsprechend wird weniger gelernt. \n",
    "##### b) Beschreibe Breite und Form der Verteilungen von der ersten bis zur letzten Schicht und erkl√§re, wie diese durch die jeweilige Transferunktion entsteht.\n",
    "###### tanh:  \n",
    "Charakterisierend ist bei dieser Verteilung, dass sie √ºber alle Schichten hinweg symmetrisch bleibt. W√§hrend in der ersten Schicht noch die urspr√ºngliche Normalverteilung im Bereich [-1, 1] erkennbar ist, wird die Verteilung √ºber die Schichten hinweg immer schmaler und spitzer. \n",
    "\n",
    "###### ReLU:  \n",
    "Auff√§llig ist hier vor allem, dass die Verteilung anfangs die vorher schon beschriebene starke Asymmetrie zugunsten des postiven Bereichs aufweist. Au√üerdem ist die Verteilung bereits in der ersten Schicht deutlich abweichend zur Normalverteilung, da sie deutlich flacher, aber daf√ºr breiter ist. Wie beim tanh auch wird die Verteilung √ºber die Schichten hinweg immer schmaler und spitzer, dennoch bleibt die Asymmetrie bis zur letzten Schicht erhalten. \n",
    "\n",
    "###### ELU:\n",
    "In der ersten Schicht zeigt sich eine Asymmetrische Normalverteilung, welche in den negativen Bereich bis ca. -1 reicht, in den positiven Bereich aber noch deutlich √ºber die 1 hinaus. Wie bei den anderen beiden Funktionen zuvor auch, wird die Verteilung mit jeder Schicht etwas schmaler und spitzer, allerdings bleibt auch in diesem Fall die Asymmetrie durch alle Schichten hindurch erhalten. \n",
    "\n",
    "###### SELU:\n",
    "Die Verteilung der ersten Schicht erinnert an eine Normalverteilung, mit der Besonderheit, dass sie weiter in den positiven als in den negativen Bereich hineinragt. Desweiteren ist besonderns, dass sich entlang der Nulllinie der Aktivierungsachse √ºber alle Schichten hinweg ein \"Graben\" ergibt. \n",
    "Im Gegensatz zu allen anderen Funktionen bleibt die Verteilung bei SELU nahezu unver√§ndert √ºber alle Schichten hinweg. Sie wird weder signifikant schmaller, noch spitzer. Allerdings bleibt auch die Asymmetrie aus der Verteilung der ersten Schicht erhalten. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
