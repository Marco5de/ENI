{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einf√ºhrung in die Neuroinformatik - 10. Aufgabenblatt\n",
    "## Gruppe Q: Dominik Authaler, Marco Deuscher, Carolin Schindler\n",
    "\n",
    "### Aufgabe 1: Transferfunktionen\n",
    "\n",
    "#### 1. Sigmoidfunktion und Tangens hyperbolicus\n",
    "##### a) Zeige, dass $sig(x) = \\frac{1+\\tanh(\\frac{x}{2})}{2}$.\n",
    "$sig(x) = \\frac{1}{1+e^{-x}} = \\frac{2\\cdot e^{\\frac{x}{2}}}{2\\cdot e^{\\frac{x}{2}} + 2\\cdot e^{-\\frac{x}{2}}} = \\frac{e^{\\frac{x}{2}} + e^{-\\frac{x}{2}} + e^{\\frac{x}{2}} - e^{-\\frac{x}{2}}}{2\\cdot (e^{\\frac{x}{2}} + e^{-\\frac{x}{2}})} = \\frac{1+\\frac{e^{\\frac{x}{2}} - e^{-\\frac{x}{2}}}{e^{\\frac{x}{2}} + e^{-\\frac{x}{2}}}}{2} = \\frac{1+\\tanh(\\frac{x}{2})}{2}$\n",
    "##### b) Unterschiede beim Lernen\n",
    "###### i) Markiere f√ºr jede Verbindung in Abbildung 4(a) f√ºr $f(x)=sig(x)$, ob der Gradient des zugeh√∂rigen Gewichts positiv oder negativ ist.\n",
    "gr√ºn: Gradient positiv, rot: Gradient negativ  \n",
    "<img src=\"Blatt10_Bild1bi.png\">\n",
    "###### ii) Wiederhole das Verfahren f√ºr das Netzwerk in Abblidung 4(b) mit $f(x) = \\tanh(x)$ als Transferfunktion.\n",
    "gr√ºn: Gradient positiv, rot: Gradient negativ  \n",
    "<img src=\"Blatt10_Bild1bii.png\">\n",
    "###### iii) Erkl√§re, wie sich die beiden Netzwerke beim Lernen unterscheiden und welchen Nachteil die Sigmoidfunktion im Vergleich zum Tangens hyperbolicus haben k√∂nnte.\n",
    "XXX\n",
    "\n",
    "#### 2. Rectified Linear Units (ReLUs)\n",
    "##### a) Welche Konsequenzen hat die Funktion auf das Lernen im Netzwerk?\n",
    "$\\forall x<0: \\text{ReLU}(x) = 0$  \n",
    "$\\forall x>0: \\text{ReLU}(x) = x$  \n",
    "D.h. f√ºr negative x ist der Gradient Null und somit bleiben die Gewichte beim Lernen unver√§ndert, obwohl diese nicht korrekt sind. Dadurch wird das Lernen verlangsamt, da nur f√ºr positive x gelernt werden kann.\n",
    "##### b) Vergleiche die Performance und die Geschwindigkeit des Lernvorgangs zwischen ReLU(x) und tanh(x).\n",
    "ReLU besitzt einen geringeren Rechenaufwand als tanh, da f√ºr tanh zwei e-Funktionen berechnet, addiert bzw. subtrahiert und geteil werden m√ºssen w√§hrend bei ReLU nur x positiv oder negativ unterschieden wird und je nachdem die Ausgabe 0 oder x ist.  \n",
    "Der Lernvorgang ist bei tanh(x) schneller, als bei ReLU(x), da bei tanh f√ºr positive und negative x gelernt wird und bei ReLU nur f√ºr positive x gerlent wird (siehe a).\n",
    "##### c) Erkl√§re, was das Problem bei dying ReLUs ist und warum eine Leaky ReLU m√∂gliche Abhilfe darstellt.\n",
    "Das Problem ist hier, dass ReLU(x) = 0 und somit wie in Teilaufgabe a) beschrieben nicht gelernt wird. Eine Leaky ReLU stellt eine m√∂gliche Abhilfe dar, da $\\forall x\\neq 0: \\text{LeakyReLU}_a(x) \\neq 0$ und somit nur nicht gelernt wird, wenn $x=0$.\n",
    "\n",
    "#### 3. Exponential Linear Unit (ELU)\n",
    "##### a) Welchen Vorteil bringt die Funktion f√ºr das Lernen im Netzwerk?\n",
    "XXX\n",
    "##### b) Scaled Exponential Linear Unit (SELU)\n",
    "###### i) Warum k√∂nnte es ung√ºnstig sein, dass die Verteilungsparameter √ºber die Lernepochen hinweg so unterschiedliche sind? Welche Nachteile ergeben sich hierbei insbesondere f√ºr die $l+1$ Schicht?\n",
    "XXX\n",
    "###### ii) Wie transformiert SELU(x) diese Werte, sodass dadurch die neue Verteilung in Abbildung 5(b) entsteht?\n",
    "XXX\n",
    "###### iii) Erkl√§re auch hier, wie es zu der neuen Verteilung in Abbildung 6(b) kommt.\n",
    "XXX\n",
    "\n",
    "#### 4. spielerisches Testen im TensorFlow-Playground\n",
    "##### a) Wie viele Epochen sind jeweils notwendig, um den Testfehler auf ca. 0.007 zu dr√ºcken?\n",
    "XXX\n",
    "##### b) Kannst du den Effekt der dying ReLUs provozieren?\n",
    "XXX\n",
    "\n",
    "#### 5. Simulationsergebnisse untersuchen\n",
    "##### a) Verteilungsparamteter f√ºr die Aktivit√§ten und die Gradienten\n",
    "###### i) Was f√§llt bez√ºglich des Bereichs auf?\n",
    "XXX\n",
    "###### ii) Wie macht sich die Asymmetrie der ReLU- und ELU-Funktionen bemerkbar?\n",
    "XXX\n",
    "###### iii) Warum ist der learning slowdown bei den Gradienten f√ºr tanh(ùë•) nicht so stark ausgepr√§gt?\n",
    "XXX\n",
    "###### iv) Mit welcher Transferfunktion wird in diesem Beispiel am meisten gelernt?\n",
    "XXX\n",
    "##### b) Beschreibe Breite und Form der Verteilungen von der ersten bis zur letzten Schicht und erkl√§re, wie diese durch die jeweilige Transferunktion entsteht.\n",
    "XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
