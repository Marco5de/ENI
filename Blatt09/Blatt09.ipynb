{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in die Neuroinformatik - 9. Aufgabenblatt\n",
    "## Gruppe Q: Dominik Authaler, Marco Deuscher, Carolin Schindler\n",
    "\n",
    "### Aufgabe 1: Gewichtsinitialisierung\n",
    "\n",
    "#### 1. Betrachten, was bei ungünstiger Initialisierung passiert\n",
    "##### a) Wie ist $u_i^{(1)}$ verteilt?\n",
    "$u_i^{(1)} = \\sum_{k=1}^m x_k\\cdot w_{ki}^{(1)} + b_i^{(1)} = \\sum_{k=1}^m x_k\\cdot N(0, 1) + 0 = \\sum_{x_k = 1} N(0,1) = N(\\frac{m}{2}\\cdot 0, \\sqrt{\\frac{m}{2}\\cdot 1^2}) = N(0, \\sqrt{\\frac{m}{2}}) = N(0, 5\\sqrt{6}) \\approx N(0, 12.25)$\n",
    "##### b) Zu welchem Problem kann das eben berechnete Ergebnis führen?\n",
    "$y_i^{(1)} = f(u_i^{(1)}) = \\tanh{u_i^{(1)}}$ und $|\\tanh{x}| = 1$ für etwa $|x| > 2$. Daher ändert sich die Ausgabe für $u_i^{(1)} > 2$ nicht, obwohl sich $u_i^{(1)}$ geändert hat. Da für N(0, 12.25) Werte von ca. -30 bis +30 wahrscheinlich sind und Werte im Intervall $[-2, 2]$ kaum wahrscheinlicher sind als Werte aushalb des Intervalls, ist die Wahrscheinlichkeit hoch, dass $|y_i^{(1)}|$ konstant $1$ ist und somit das Lernen keinen Effekt zeigt.\n",
    "##### c) Wie ist $u_i{(1)}$ mit dieser Konfiguration verteilt?\n",
    "$u_i^{(1)} = N(\\frac{m}{2}\\cdot 0, \\sqrt{\\frac{m}{2}\\cdot \\frac{1}{n_{in}}}) = N(0, \\sqrt{\\frac{m}{2\\cdot m}}) = N(0, \\sqrt{\\frac{1}{2}}) \\approx N(0, 0.707)$\n",
    "##### d) Welche Vorteile hat diese Verteilung für die Initialisierung der Gewichte im Vergleich zu einer unveränderten Normalisierung?\n",
    "Der Vorteil liegt darin, dass z.B. hier Werte außerhalb des Intervall $[-2, 2]$ mit einer Wahrscheinlichkeit von 0%, also nie auftreten und dafür die anderen Werte, insbesondere in der Nähe von Null, mit einer höhren Wahrscheinlichkeit auftreten. Dadurch tritt das oben beschrieben Problem auch nicht im kleinen Stil bzw. durch \"unglücklichen Zufall\" auf. \n",
    "\n",
    "#### 2. Welche Vertelung passt zur Initialierungsstrategier $I$ und welche zu $II$?\n",
    "A: Standardnormalverteilung $N(0,1)$    \n",
    "B: skalierte Standardnormalverteilung $N(0, \\frac{1}{\\sqrt{n_{in}}})$  \n",
    "Für A ist $|y_i^{(1)}| \\approx 1$ am wahrscheinlichsten und da $y_i^{(1)} = f(u_i^{(1)}) = \\tanh{u_i^{(1)}}$, muss $|u_i^{(1)}| > 2$ sehr wahrscheinlich sein. Für B sind die Werte von $y_i^{(1)}$ im Intervall $[-1, 1]$ ziemlich gleichverteilt, daher muss $|u_i^{(1)}| < 2$ sein und für Werte um Null etwa die gleiche Auftrittswahrscheinlichkeit besitzen. Daher ergibt sich oben genannte Zuordnung.\n",
    "\n",
    "#### 3. Erkläre, welches Problem man damit zu lösen versucht und warum die Gleichung einen möglichen Kompromiss darstellt. \n",
    "XXX\n",
    "\n",
    "### Aufgabe 2: Regularisierung\n",
    "\n",
    "#### 1. veränderte Kostenfunktion\n",
    "##### a) Zeige, dass sich für ein einzelnes Gewicht die neue Lernregel ergibt.\n",
    "$\\frac{\\partial E(w(t))}{\\partial w_{ij}^{(l)}} = \\frac{\\partial E_0(w(t))}{\\partial w_{ij}^{(l)}} + \\frac{\\partial}{\\partial w_{ij}^{(l)}} (\\frac{\\lambda}{2}\\sum_{l=1}^{L}\\sum_{i}\\sum_{j}(w_{ij}^{(l)})^2) = \\frac{\\partial E_0(w(t))}{\\partial w_{ij}^{(l)}} + \\lambda\\cdot w_{ij}^{(l)}$  \n",
    "$w(t+1) = w(t) -\\eta\\cdot\\nabla E(w(t)) = w(t) -\\eta\\cdot (\\nabla E_0(w(t)) + \\lambda\\cdot w(t)) = (1-\\eta\\cdot\\lambda)\\cdot w(t) - \\eta\\cdot\\nabla E_0(w(t))$\n",
    "##### b) Gewichte mit weight decacy\n",
    "###### i) Welchen Wert nimmt das Gewicht bei $w(10)$ an?\n",
    "$w(t+1) = (1-0,8\\cdot0,5)\\cdot w(t) = 0,6\\cdot w(t)$  \n",
    "$w(10) = 0,6\\cdot w(9) = 0,6\\cdot(0,6\\cdot w(8)) = 0,6^{10}\\cdot w(0) \\approx 0,0121$  \n",
    "[$w(t+1) = 0,6^{t+1}\\cdot w(0)$]\n",
    "###### ii) Mit welcher Rate nimmt das Gewicht ab?\n",
    "Das Gewicht nimmt exponentiell ab.\n",
    "###### iii) Zeige, dass sich das Gewicht in zunehmenden Iterationen der Null annähert.\n",
    "$\\lim_{t \\to \\infty} w(t) = \\lim_{t \\to \\infty} 0,6^{t}\\cdot w(0) = 2\\cdot\\lim_{t \\to \\infty} 0,6^{t} = 2\\cdot 0 = 0$\n",
    "###### iv) Argumentiere, warum die Gleichung nun nicht mehr notwendigerweise gilt.\n",
    "XXX\n",
    "##### c) Wenn wir nun jedoch die Lernregel mit Regularisierung verwenden, hilft uns das dann dabei, das Problem aus Aufgabe 1 einzudämmen und wenn ja warum?\n",
    "XXX\n",
    "\n",
    "#### 2. Behauptung stützen\n",
    "XXX\n",
    "\n",
    "#### 3. Skizziere im zweidimensionalen Raum wie sich die Fehlerfunktion für steigendes $\\lambda$ verhält.\n",
    "XXX\n",
    "\n",
    "#### 4. Effekt der Regularisierung anhand eines Beispiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XXX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e9829dade586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mXXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'XXX' is not defined"
     ]
    }
   ],
   "source": [
    "XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
